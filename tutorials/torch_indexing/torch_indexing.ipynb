{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7608de8d",
   "metadata": {},
   "source": [
    "# Torch Indexing\n",
    "\n",
    "Master **PyTorch indexing** to access, slice, and modify tensors like a pro âš¡\n",
    "\n",
    "For each problem, weâ€™ll start with a **naÃ¯ve Python solution** and the goal is to refactor it using **PyTorch** to make it blazingly fast ðŸš€ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0099ac",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74892e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a10d61e",
   "metadata": {},
   "source": [
    "## Problem 1 : Genetic Algorithm\n",
    "\n",
    "The goal is to learn the weights of a small neural network using a genetic algorithm.\n",
    "\n",
    "We will simulate **mutation**, **crossover**, and **natural selection** to evolve the network and find the best-performing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d06ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genetic_env import Simulator, TorchCartPole, TorchPendulum\n",
    "\n",
    "def mutation_noise_py(pop, mutation_rate=0.1, mutation_scale=0.3):\n",
    "    \"\"\"\n",
    "    Add noise to random weights of the population.\n",
    "    \n",
    "    pop : tensor (num_agent, k)\n",
    "    mutation_rate : chance of adding noise to each weights\n",
    "    mutation_scale : std of the noise added\"\"\"\n",
    "    (n, k) = pop.shape\n",
    "    for i in range(n):\n",
    "        for j in range(k):\n",
    "            if random.random() < mutation_rate:\n",
    "                pop[i, j] += random.gauss(0, mutation_scale)\n",
    "    return pop\n",
    "\n",
    "\n",
    "def mutation_noise(pop, mutation_rate=0.1, mutation_scale=0.3):\n",
    "    \"\"\"\n",
    "    Add noise to random weights of the population.\n",
    "    \n",
    "    pop : tensor (num_agent, k)\n",
    "    mutation_rate : chance of adding noise to each weights\n",
    "    mutation_scale : std of the noise added\"\"\"\n",
    "    mask = torch.rand_like(pop) < mutation_rate\n",
    "    noise = torch.randn_like(pop) * mutation_scale\n",
    "    pop = pop + mask*noise\n",
    "    return pop\n",
    "\n",
    "def crossover_merge(pop, number_new):\n",
    "    n, k = pop.shape\n",
    "    # Randomly select pairs of parents\n",
    "    parents_idx = torch.randint(0, n, (number_new, 2))\n",
    "    \n",
    "    parents = pop[parents_idx]\n",
    "    \n",
    "    # Create a mask of shape (number_new x k)\n",
    "    mask = (torch.rand(number_new, k) < 0.5).to(pop.device)\n",
    "    \n",
    "    # parent0 genes where mask is True, else parent1 genes\n",
    "    child = torch.where(mask, parents[:, 0, :], parents[:, 1, :])\n",
    "    \n",
    "    return child\n",
    "\n",
    "def crossover_merge_py(pop, number_new):\n",
    "    \"\"\"\n",
    "    randomly merge weights of elements 2 by 2 of the population\n",
    "    \"\"\"\n",
    "    (n, k) = pop.shape\n",
    "    pop_new = torch.zeros((number_new, k))\n",
    "    for l in range(number_new):\n",
    "        indexes = torch.randint(0, n, (2,))\n",
    "        for j in range(k):\n",
    "            if random.random() < 0.5:\n",
    "                pop_new[l, j] = pop[indexes[0], j]\n",
    "            else:\n",
    "                pop_new[l, j] = pop[indexes[1], j]\n",
    "    return pop_new\n",
    "\n",
    "def hard_natural_selection_py(pop, rewards, survival_rate=0.5):\n",
    "    n = pop.shape[0]\n",
    "    num_survivors = int(n * survival_rate)\n",
    "\n",
    "    rewards_list = [rewards[i].item() for i in range(n)]\n",
    "\n",
    "    # Sorted indices of individual of population from best reward to worst reward\n",
    "    sorted_indices = sorted(range(n), key=lambda i: rewards_list[i], reverse=True)\n",
    "\n",
    "    # Pick top survivors\n",
    "    survivors= []\n",
    "    for i in range(num_survivors):\n",
    "        survivors.append(pop[sorted_indices[i]])\n",
    "    return torch.stack(survivors)\n",
    "\n",
    "def hard_natural_selection(pop, rewards, survival_rate=0.5):\n",
    "    n = pop.shape[0]\n",
    "    num_survivors = int(n * survival_rate)\n",
    "    # Get top survivor indices sorted by descending rewards\n",
    "    \n",
    "    _, sorted_indices = torch.sort(rewards, descending=True)\n",
    "    # Select top survivors directly\n",
    "    survivors = pop[sorted_indices[:num_survivors]]\n",
    "    return survivors\n",
    "\n",
    "def genetic_algorithm(env:Simulator, pop, steps=50, device=\"cpu\", verbose=True, survival_rate=0.1, mutation_rate=0.3, mutation_scale=0.5):\n",
    "    \"\"\" \n",
    "    Apply genetic algorithm to optimize the population's reward on the environment\n",
    "    \"\"\"\n",
    "    pop_size = len(pop)\n",
    "    pop = pop.to(device)\n",
    "    rewards = env.evaluate(pop).to(device)\n",
    "    log = []\n",
    "    enum = range(steps)\n",
    "    if verbose:\n",
    "        enum = tqdm.tqdm(enum)\n",
    "    for _ in enum:\n",
    "        survivors = hard_natural_selection(pop, rewards, survival_rate=survival_rate)\n",
    "        best_indiv = survivors[0].unsqueeze(0) # keep the best individual at each step to have only increasing reward\n",
    "        nomber_of_ind_to_add = pop_size - len(survivors)\n",
    "        crossed_indviduals = crossover_merge(survivors, nomber_of_ind_to_add).to(device)\n",
    "        pop = torch.cat([crossed_indviduals, survivors], dim=0).to(device)\n",
    "        pop = mutation_noise(pop, mutation_rate=mutation_rate, mutation_scale=mutation_scale)\n",
    "        pop[-1] = best_indiv\n",
    "        \n",
    "        rewards = env.evaluate(pop).to(device)\n",
    "        # logging \n",
    "        best_idx = rewards.argmax()\n",
    "        log.append((rewards[best_idx], pop[best_idx]))\n",
    "    \n",
    "    return pop, log\n",
    "    \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pop_size = 10\n",
    "env = TorchPendulum(device)\n",
    "pop = torch.randn((pop_size, env.weight_size))\n",
    "pop_optimized, log = genetic_algorithm(env, pop, steps=50, device=device, survival_rate=0.1)\n",
    "best_indiv = log[-1][1]\n",
    "env.record_run(best_indiv, video_path=\"best_pendulum.mp4\")\n",
    "# contains all best individual reward at each optimization step\n",
    "best_rewards_per_step = [entry[0].item() for entry in log]\n",
    "plt.plot(best_rewards_per_step)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Best Reward')\n",
    "plt.title('Best Reward over Genetic Algorithm Steps')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0160ac3",
   "metadata": {},
   "source": [
    "## Problem 2 : Particule Swarm Optimization\n",
    "\n",
    "The goal is to learn the weights of a small neural network using a particule swarm optimization algorithm.\n",
    "\n",
    "We will simulate **mutation**, **crossover**, and **natural selection** to evolve the network and find the best-performing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb17c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "\n",
    "def particle_swarm_optimization(env, swarm_size=50, steps=100, device=\"cpu\",\n",
    "                                 w=0.5, phi_p=1.5, phi_g=1.5,\n",
    "                                 verbose=True):\n",
    "    \"\"\"\n",
    "    Particle Swarm Optimization (PSO) for maximizing environment reward.\n",
    "    No boundaries â€” uses Gaussian initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Random normal initialization for positions & velocities\n",
    "    x = torch.randn((swarm_size, env.weight_size), device=device)\n",
    "    v = torch.randn_like(x)\n",
    "\n",
    "    # Evaluate initial positions\n",
    "    rewards = env.evaluate(x)\n",
    "    p = x.clone()                          # personal best positions\n",
    "    p_rewards = rewards.clone()            # personal best rewards\n",
    "\n",
    "    # Global best\n",
    "    g_idx = p_rewards.argmax()\n",
    "    g = p[g_idx].clone()\n",
    "    g_reward = p_rewards[g_idx].clone()\n",
    "\n",
    "    log = []\n",
    "    enum = range(steps)\n",
    "    if verbose:\n",
    "        enum = tqdm.tqdm(enum)\n",
    "\n",
    "    for _ in enum:\n",
    "        # Evaluate current positions\n",
    "        rewards = env.evaluate(x)\n",
    "\n",
    "        # Update personal bests\n",
    "        improved = rewards > p_rewards\n",
    "        p[improved] = x[improved]\n",
    "        p_rewards[improved] = rewards[improved]\n",
    "\n",
    "        # Update global best\n",
    "        best_idx = p_rewards.argmax()\n",
    "        if p_rewards[best_idx] > g_reward:\n",
    "            g = p[best_idx].clone()\n",
    "            g_reward = p_rewards[best_idx].clone()\n",
    "\n",
    "        # Update velocities & positions\n",
    "        rp = torch.rand_like(x)\n",
    "        rg = torch.rand_like(x)\n",
    "        v = (w * v\n",
    "             + phi_p * rp * (p - x)\n",
    "             + phi_g * rg * (g.unsqueeze(0) - x))\n",
    "        x = x + v\n",
    "\n",
    "        # Log best reward so far\n",
    "        log.append((g_reward.clone(), g.clone()))\n",
    "\n",
    "    return g, log\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pop_size = 1000\n",
    "env = TorchPendulum(device)\n",
    "pop = torch.randn((pop_size, env.weight_size))\n",
    "pop_optimized, log = genetic_algorithm(env, pop, steps=50, device=device, survival_rate=0.1)\n",
    "best_indiv = log[-1][1]\n",
    "env.record_run(best_indiv, video_path=\"best_pendulum.mp4\")\n",
    "# contains all best individual reward at each optimization step\n",
    "best_rewards_per_step = [entry[0].item() for entry in log]\n",
    "plt.plot(best_rewards_per_step)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Best Reward')\n",
    "plt.title('Best Reward over Genetic Algorithm Steps')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7afedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem : Particule Swarm Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ebd977",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem : Graph message passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d95f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
